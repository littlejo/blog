+++
date = '2025-03-17T21:35:52+01:00'
draft = false
title = 'Cilium avec EKS : Introduction'
+++

# Introduction

Nous allons voir Cilium sous toutes ces facettes avec EKS, comment il sâ€™intÃ¨gre Ã  lâ€™environnement dâ€™AWS. Pour cette premiÃ¨re partie, on va voir ce quâ€™est Cilium et EKS et comment installer rapidement Cilium sur un cluster EKS.

---

# Quâ€™est-ce quâ€™AWS EKS et Cilium ?

AWS EKS est un service gÃ©rÃ© par AWS. Il permet de crÃ©er des clusters Kubernetes. Kubernetes est un orchestrateur de containers. Jâ€™ai choisi ce service car je le connais bien et que Ã§a permet de confronter cilium Ã  un â€œvraiâ€ cluster et non Ã  des clusters de dev comme kind ou minikube.

Cilium est un plugin rÃ©seau (ou CNI : Container Network Interface) pour Kubernetes. Ce projet est gÃ©rÃ© par la sociÃ©tÃ© Isovalent. Contrairement Ã  la plupart des autres CNI qui utilisent iptables/netfilter pour gÃ©rer les modifications rÃ©seaux, Cilium utilise lâ€™eBPF. Pour faire simple, lâ€™eBPF a beaucoup dâ€™avantages sur iptables. Par exemple, la performance est meilleure, lâ€™identification des trames rÃ©seaux est plus simple qui permet ainsi dâ€™avoir une meilleure observabilitÃ© du rÃ©seau. Ainsi Isovalent a crÃ©Ã© un outil qui sâ€™appelle Hubble permettant de visualiser les flux rÃ©seaux dans le rÃ©seau cilium Ã  lâ€™instar de tcpdump dans un modÃ¨le traditionnel.

Par dÃ©faut, AWS EKS utilise un autre plugin rÃ©seau AWS VPC CNI. Pourquoi utiliser Cilium plutÃ´t que le plugin par dÃ©faut ? Nous allons le voir pendant tout cet Ã©tÃ© mais la premiÃ¨re chose qui me vient en tÃªte est lâ€™absence de Network Policies, des rÃ¨gles qui permettent de dire je veux que tel pod accÃ¨de Ã  tel pod via tel port. Cela nâ€™est pas possible par exemple avec le CNI par dÃ©faut. Nous allons aussi explorer des limitations au niveau de Cilium. Tout nâ€™est pas rose non plus.

Pour ce premier Ã©pisode, nous allons faire (trÃ¨s) simple : on va dÃ©ployer un cluster eks et installer cilium.

---

# PrÃ©-requis

* un compte AWS avec des access keys
* un peu dâ€™argent (Pour une heure : 0.1 $ pour le cluster eks, environ 0.08 $ pour deux t3.medium et 0.05 $ pour la nat gateway). DurÃ©e minimale : environ 30 minutes
* eksctl : outil pour dÃ©ployer des clusters eks
* aws iam authenticator : outil pour sâ€™authentifier auprÃ¨s du cluster eks
* aws cli : outil pour communiquer avec lâ€™API dâ€™AWS
* kubectl : outil pour controler le cluster kubernetes
* cilium cli : outil pour installer cilium

Comme AWS EKS est un service payant, il est conseillÃ© dâ€™avoir bien installÃ© les outils avant de crÃ©er le cluster.

---

# DÃ©ploiement dâ€™un cluster AWS EKS

Nous allons voir comment dÃ©ployer rapidement un cluster AWS EKS.

On va exporter les access keys :

```bash
export AWS_DEFAULT_REGION=ch-ange-1
export AWS_ACCESS_KEY_ID="CHANGEME"
export AWS_SECRET_ACCESS_KEY="CHANGEME"
```

On va crÃ©er un fichier yaml pour eksctl:

```yaml
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: basic-cilium
  region: us-east-1
  version: "1.27"

managedNodeGroups:
- name: ng-1
  instanceType: t3.medium
  # taint nodes so that application pods are
  # not scheduled/executed until Cilium is deployed.
  # Alternatively, see the note above regarding taint effects.
  taints:
   - key: "node.cilium.io/agent-not-ready"
     value: "true"
     effect: "NoExecute"
```

Je dÃ©ploie sur us-east-1 mais si vous prÃ©fÃ©rez utilisez une autre rÃ©gion, nâ€™hÃ©sitez pas Ã  changer.

On va lancer la commande suivante :

```bash
eksctl create cluster -f ./files/eks-cilium.yaml
```

Cela va prendre un temps important (de lâ€™ordre de 15 minutes).

Voici ce que cette commande va crÃ©er :

Une fois que câ€™est fini, on va pouvoir lancer des commandes kubectl :

```
kubectl get node
NAME STATUS ROLES AGE VERSION
ip-192â€“168â€“11â€“135.ec2.internal Ready <none> 4m18s v1.27.1-eks-2f008fe
ip-192â€“168â€“56â€“129.ec2.internal Ready <none> 4m22s v1.27.1-eks-2f008fe
```

---

# Installation de Cilium

Rien de plus simple avec la cli cilium :

```
cilium install
ğŸ”® Auto-detected Kubernetes kind: EKS
â„¹ï¸  Using Cilium version 1.13.3
ğŸ”® Auto-detected cluster name: basic-cilium-us-east-1-eksctl-io
ğŸ”® Auto-detected datapath mode: aws-eni
ğŸ”® Auto-detected kube-proxy has been installed
ğŸ”¥ Patching the "aws-node" DaemonSet to evict its pods...
â„¹ï¸  helm template --namespace kube-system cilium cilium/cilium --version 1.13.3 --set cluster.id=0,cluster.name=basic-cilium-us-east-1-eksctl-io,egressMasqueradeInterfaces=eth0,encryption.nodeEncryption=false,eni.enabled=true,ipam.mode=eni,kubeProxyReplacement=disabled,operator.replicas=1,serviceAccounts.cilium.name=cilium,serviceAccounts.operator.name=cilium-operator,tunnel=disabled
â„¹ï¸  Storing helm values file in kube-system/cilium-cli-helm-values Secret
ğŸ”‘ Created CA in secret cilium-ca
ğŸ”‘ Generating certificates for Hubble...
ğŸš€ Creating Service accounts...
ğŸš€ Creating Cluster roles...
ğŸš€ Creating ConfigMap for Cilium version 1.13.3...
ğŸš€ Creating Agent DaemonSet...
ğŸš€ Creating Operator Deployment...
âŒ› Waiting for Cilium to be installed and ready...
âœ… Cilium was successfully installed! Run 'cilium status' to view installation health
```

On voit que cilium dÃ©tecte pas mal de chose automatiquement, par exemple quâ€™il va installer sur EKS. Il va installer la version 1.13.3. Il va donc supprimer les pods qui composent lâ€™aws vpc cni. Il va ensuite gÃ©nÃ©rer toutes les dÃ©pendances permettant dâ€™installer cilium sur le cluster.

La commande cilium status va permettre dâ€™avoir un aperÃ§u si cela a bien Ã©tÃ© installÃ© :

```
cilium status --wait
    /Â¯Â¯\
 /Â¯Â¯\__/Â¯Â¯\    Cilium:             OK
 \__/Â¯Â¯\__/    Operator:           OK
 /Â¯Â¯\__/Â¯Â¯\    Envoy DaemonSet:    disabled (using embedded mode)
 \__/Â¯Â¯\__/    Hubble Relay:       disabled
    \__/       ClusterMesh:        disabled

Deployment        cilium-operator    Desired: 1, Ready: 1/1, Available: 1/1
DaemonSet         cilium             Desired: 2, Ready: 2/2, Available: 2/2
Containers:       cilium-operator    Running: 1
                  cilium             Running: 2
Cluster Pods:     2/2 managed by Cilium
Image versions    cilium             quay.io/cilium/cilium:v1.13.3@sha256:77176464a1e11ea7e89e984ac7db365e7af39851507e94f137dcf56c87746314: 2
                  cilium-operator    quay.io/cilium/operator-aws:v1.13.3@sha256:394c40d156235d3c2004f77bb73402457092351cc6debdbc5727ba36fbd863ae: 1
```

On rajoute lâ€™option wait pour attendre que lâ€™installation soit bien finie.

VÃ©rifions maintenant que lâ€™installation sâ€™est bien passÃ©e :

```
cilium connectivity test
```

La commande va faire plein de tests rÃ©seaux. Cela va donc prendre un temps important.

En exclusivitÃ© voici le rÃ©sumÃ© final du test :
```
ğŸ“‹ Test Report
âŒ 4/42 tests failed (20/300 actions), 12 tests skipped, 1 scenarios skipped:
Test [no-policies]:
  âŒ no-policies/pod-to-host/ping-ipv4-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.243.4.228 (54.243.4.228:0)
  âŒ no-policies/pod-to-host/ping-ipv4-3: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.158.35.146 (54.158.35.146:0)
  âŒ no-policies/pod-to-host/ping-ipv4-5: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.158.35.146 (54.158.35.146:0)
  âŒ no-policies/pod-to-host/ping-ipv4-7: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.243.4.228 (54.243.4.228:0)
Test [no-policies-extra]:
  âŒ no-policies-extra/pod-to-remote-nodeport/curl-0: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-same-node (echo-same-node:8080)
  âŒ no-policies-extra/pod-to-remote-nodeport/curl-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-other-node (echo-other-node:8080)
  âŒ no-policies-extra/pod-to-remote-nodeport/curl-2: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-other-node (echo-other-node:8080)
  âŒ no-policies-extra/pod-to-remote-nodeport/curl-3: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-same-node (echo-same-node:8080)
  âŒ no-policies-extra/pod-to-local-nodeport/curl-0: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-other-node (echo-other-node:8080)
  âŒ no-policies-extra/pod-to-local-nodeport/curl-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> cilium-test/echo-same-node (echo-same-node:8080)
  âŒ no-policies-extra/pod-to-local-nodeport/curl-2: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-same-node (echo-same-node:8080)
  âŒ no-policies-extra/pod-to-local-nodeport/curl-3: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> cilium-test/echo-other-node (echo-other-node:8080)
Test [allow-all-except-world]:
  âŒ allow-all-except-world/pod-to-host/ping-ipv4-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.158.35.146 (54.158.35.146:0)
  âŒ allow-all-except-world/pod-to-host/ping-ipv4-3: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.243.4.228 (54.243.4.228:0)
  âŒ allow-all-except-world/pod-to-host/ping-ipv4-5: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.158.35.146 (54.158.35.146:0)
  âŒ allow-all-except-world/pod-to-host/ping-ipv4-7: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.243.4.228 (54.243.4.228:0)
Test [host-entity]:
  âŒ host-entity/pod-to-host/ping-ipv4-1: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.158.35.146 (54.158.35.146:0)
  âŒ host-entity/pod-to-host/ping-ipv4-3: cilium-test/client-6965d549d5-hwmbt (192.168.61.62) -> 54.243.4.228 (54.243.4.228:0)
  âŒ host-entity/pod-to-host/ping-ipv4-5: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.158.35.146 (54.158.35.146:0)
  âŒ host-entity/pod-to-host/ping-ipv4-7: cilium-test/client2-76f4d7c5bc-5h4xr (192.168.63.171) -> 54.243.4.228 (54.243.4.228:0)
```

On voit dÃ©jÃ  que :

* le ping des pods vers les hosts ne fonctionne pas.
* le test des nodeports ne fonctionne pas

Câ€™est dÃ©jÃ  pas mal, on nâ€™a pas forcÃ©ment besoin du ping ni des nodeports.

Si on veut rÃ©soudre le problÃ¨me, il suffit dâ€™ouvrir le ping et les ports tcp des nodeports (30000â€“32767) au niveau du security group qui concernent les ec2. On aura alors :

On aura alors :

```
âœ… All 42 tests (300 actions) successful, 12 tests skipped, 1 scenarios skipped.
```

---

La premiÃ¨re partie est finie. Vous pouvez voir cette installation rÃ©sumÃ©e ici : https://github.com/littlejo/cilium-eks-cookbook/blob/main/install-cilium-eks.md

Dans la prochaine partie, nous verrons ce qui est â€œcachÃ©â€ dans lâ€™installation avec la cli cilium avec lâ€™installation avec helm.
